{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Vibhor\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import google.generativeai as genai  # Importing the Google Generative AI module from the google package\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting the API key for Google Generative AI service by assigning it to the environment variable 'GOOGLE_API_KEY'\n",
        "api_key = os.environ['GOOGLE_API_KEY'] = \"xx-xxxxx\"\n",
        "\n",
        "# Configuring Google Generative AI module with the provided API key\n",
        "genai.configure(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ByPDcXY9Puun"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    # Class method to instantiate a model\n",
        "    @classmethod\n",
        "    def model(cls,model_name=None,temp=None):\n",
        "        try:\n",
        "            # Attempt to initialize and return a ChatGoogleGenerativeAI model\n",
        "            return ChatGoogleGenerativeAI(model=model_name,temperature=temp, google_api_key=api_key)\n",
        "        except Exception as e:\n",
        "            # Return any exception that occurs during instantiation\n",
        "            return e\n",
        "\n",
        "class Chain:\n",
        "    # Class method to create a chain\n",
        "    @classmethod\n",
        "    def chain(cls, model_name=None, temp=None,prompt=None):\n",
        "        try:\n",
        "            # Attempt to create an LLMChain instance using the specified model and prompt\n",
        "            return LLMChain(llm=Model.model(model_name,temp),prompt=prompt)\n",
        "        except Exception as e:\n",
        "            # Return any exception that occurs during chain creation\n",
        "            return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4hA2TJqOD_li"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PromptTemplates:\n",
        "    \"\"\"\n",
        "    This class provides reusable prompt templates for different prompting strategies:\n",
        "    1. Keyword Extraction (RAG style factual QA)\n",
        "    2. Chain-of-Thought reasoning (step-by-step logical answering)\n",
        "    3. Verification prompts (double-check factual correctness)\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def key_word_extraction(cls):\n",
        "        \"\"\"\n",
        "        Use Case:\n",
        "        ---------\n",
        "        Ideal for **RAG (Retrieval-Augmented Generation)** tasks.\n",
        "        It ensures that the model answers strictly based on retrieved document context\n",
        "        without adding external knowledge or assumptions.\n",
        "\n",
        "        Example Usage:\n",
        "        - Document Question Answering.\n",
        "        - Information extraction tasks where accuracy is critical.\n",
        "        - Preventing hallucinations in factual QA.\n",
        "\n",
        "        Returns:\n",
        "            A PromptTemplate instructing the model to:\n",
        "            - ONLY use facts explicitly from context.\n",
        "            - Avoid external knowledge/assumptions.\n",
        "            - Reply explicitly if the answer is not present.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = \"\"\"\n",
        "                    You are an intelligent assistant.\n",
        "\n",
        "                    Below is the information retrieved from the document:\n",
        "\n",
        "                    {context}\n",
        "\n",
        "                    Now, answer the following question strictly based on the above information:\n",
        "\n",
        "                    Question: {question}\n",
        "\n",
        "                    Guidelines:\n",
        "                    - ONLY use facts explicitly mentioned in the context.\n",
        "                    - Do NOT use external knowledge.\n",
        "                    - Do NOT make assumptions.\n",
        "                    - If the answer is not present, reply: \"The document does not contain this information.\"\n",
        "\n",
        "                    Provide your answer:\n",
        "                    \"\"\"\n",
        "            return PromptTemplate(template=prompt.strip(), input_variables=[\"context\", \"question\"])\n",
        "        except Exception as e:\n",
        "            return e\n",
        "\n",
        "    @classmethod\n",
        "    def chain_of_thoughts(cls):\n",
        "        \"\"\"\n",
        "        Use Case:\n",
        "        ---------\n",
        "        Implements **Chain-of-Thought (CoT) prompting**.\n",
        "        Helps model break down complex questions and reason step by step logically based ONLY on the document content.\n",
        "\n",
        "        Example Usage:\n",
        "        - Complex document QA needing logical reasoning.\n",
        "        - Scenarios where multi-step analysis improves answer accuracy.\n",
        "        - Summarization tasks requiring explanation or derivation.\n",
        "\n",
        "        Returns:\n",
        "            A PromptTemplate guiding the model to:\n",
        "            - Think step by step.\n",
        "            - Extract, analyze, and logically derive the answer.\n",
        "            - Avoid external knowledge and assumptions.\n",
        "            - Avoid bullet points; present a smooth, clear final answer.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = \"\"\"\n",
        "                       You are a thoughtful assistant.\n",
        "\n",
        "                        Here is the document content:\n",
        "\n",
        "                        {context}\n",
        "\n",
        "                        Question: {question}\n",
        "\n",
        "                        Think step by step based ONLY on the provided content.\n",
        "                        Extract the relevant information, analyze it logically, and derive a clear answer.\n",
        "\n",
        "                        Rules:\n",
        "                        - Do NOT use any outside knowledge.\n",
        "                        - Do NOT assume facts not explicitly stated.\n",
        "                        - Just only display and elaborate the final answer.\n",
        "\n",
        "                        Begin reasoning:\n",
        "                    \"\"\"\n",
        "            return PromptTemplate(template=prompt.strip(), input_variables=[\"context\", \"question\"])\n",
        "        except Exception as e:\n",
        "            return e\n",
        "\n",
        "    @classmethod\n",
        "    def verification_prompt(cls):\n",
        "        \"\"\"\n",
        "        Use Case:\n",
        "        ---------\n",
        "        Implements **Verification prompting technique**.\n",
        "        Helps double-check the factual correctness of model-generated answers by verifying if they are supported by the document content.\n",
        "\n",
        "        Example Usage:\n",
        "        - Post-processing stage after answer generation to ensure factual alignment.\n",
        "        - Use in sensitive domains like healthcare, legal, finance.\n",
        "        - To flag unverifiable or hallucinated answers.\n",
        "\n",
        "        Returns:\n",
        "            A PromptTemplate that:\n",
        "            - Cross-verifies if the provided answer is directly supported by document context.\n",
        "            - Gives a binary \"Verified\" or \"Cannot verify\" response.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            prompt = \"\"\"\n",
        "                        You are a careful assistant.\n",
        "\n",
        "                        Here is the document content:\n",
        "\n",
        "                        {context}\n",
        "\n",
        "                        Question: {question}\n",
        "\n",
        "                        Provide the answer **based ONLY on the above document content**.\n",
        "\n",
        "                        Verify if the answer can be **directly supported by the content**.\n",
        "                        - If YES, state: \"Verified: Answer supported by the document.\"\n",
        "                        - If NO, state: \"Cannot verify: The document does not contain enough information.\"\n",
        "\n",
        "                        Answer:\n",
        "                    \"\"\"\n",
        "            return PromptTemplate(template=prompt.strip(), input_variables=[\"context\", \"question\"])\n",
        "        except Exception as e:\n",
        "            return e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConversationAgent:\n",
        "    # Constructor to initialize the ConversationAgent with model name and temperature\n",
        "    def __init__(self, model_name=None, temp=None):\n",
        "        self.model = model_name\n",
        "        self.temperature = temp\n",
        "    \n",
        "    # Method to declare chains based on a given prompt template\n",
        "    def declare_chains(self, prompt_template):\n",
        "        try:\n",
        "            # Instantiate a chain using the specified model and prompt template\n",
        "            return Chain.chain(model_name=self.model, temp=self.temperature, prompt=prompt_template)\n",
        "        except Exception as e:\n",
        "            # Return any exception that occurs during chain declaration\n",
        "            return e\n",
        "    \n",
        "    # Method to run the conversation agent with user input and prompt\n",
        "    def run(self, user_text=None, prompt=None):\n",
        "        # Invoke the declared chain with user input and return the response\n",
        "        response = self.declare_chains(prompt).invoke(user_text)\n",
        "        return response['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ConversationAgent\n",
        "agent = ConversationAgent(model_name=\"gemini-1.5-flash\", temp=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Vibhor\\AppData\\Local\\Temp\\ipykernel_14824\\3967111467.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  return LLMChain(llm=Model.model(model_name,temp),prompt=prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gustave Eiffel\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample context and question\n",
        "context = \"\"\"The Eiffel Tower is one of the most famous landmarks in Paris, France. It was designed by Gustave Eiffel and completed in 1889.\"\"\"\n",
        "question = \"Who designed the Eiffel Tower?\"\n",
        "\n",
        "# Define your RAG Prompt Template\n",
        "rag_prompt = PromptTemplates.key_word_extraction()\n",
        "# Prepare input variables\n",
        "user_input = {\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "}\n",
        "\n",
        "# Run\n",
        "response = agent.run(user_text=user_input, prompt=rag_prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John will check out of the hotel on March 28th.  This is because he booked a hotel for 3 nights starting on March 25th (the day he arrives).  Three nights after March 25th is March 28th.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example document content and question\n",
        "document_content = \"\"\"\n",
        "John is planning to attend a conference in New York. He booked his flight for March 25th and reserved a hotel for 3 nights.\n",
        "\"\"\"\n",
        "document_question = \"On what date will John check out from the hotel?\"\n",
        "user_input = {\n",
        "    \"context\": document_content,\n",
        "    \"question\": document_question\n",
        "}\n",
        "# Define COT Prompt Template\n",
        "cot_prompt = PromptTemplates.chain_of_thoughts()\n",
        "\n",
        "# Run\n",
        "response = agent.run(user_text=user_input, prompt=cot_prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
